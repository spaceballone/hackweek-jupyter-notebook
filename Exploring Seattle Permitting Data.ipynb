{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Seattle Building Permit Data in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run this notebook in Google Colab, do the following:\n",
    "\n",
    "1. Navigate to Google Colab: https://colab.research.google.com\n",
    "2. Choose \"Open notebook\" from the File menu\n",
    "3. Click on the Github menu option\n",
    "2. Copy in this URL: https://github.com/rlvoyer/jupyter-notebooks/blob/master/tyler-connect\n",
    "\n",
    "Before we get started with any data, we need to make sure our environment has all the Python dependencies we need. In the cell below, we'll use the `pip install` command to install those dependencies. Typically, you might run this command directly from a command-line in a terminal application. You can get the same effect from within a Jupyter notebook by prepending an exaclamation point `!` to that command, which indicates to the interpreter that the command should be passed through to the shell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: wget: command not found\n",
      "tar: Error opening archive: Failed to open 'spatialindex-src-1.8.5.tar.gz'\n",
      "/bin/sh: line 0: cd: spatialindex-src-1.8.5: No such file or directory\n",
      "/bin/sh: line 0: cd: spatialindex-src-1.8.5: No such file or directory\n",
      "/bin/sh: ldconfig: command not found\n",
      "WARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.\n",
      "Please see https://github.com/pypa/pip/issues/5599 for advice on fixing the underlying issue.\n",
      "To avoid this problem you can invoke Python with '-m pip' instead of running pip directly.\n",
      "Requirement already satisfied: pandas==0.24.2 in /usr/local/lib/python3.7/site-packages (0.24.2)\n",
      "Requirement already satisfied: statsmodels==0.9.0 in /usr/local/lib/python3.7/site-packages (0.9.0)\n",
      "Requirement already satisfied: rtree in /usr/local/lib/python3.7/site-packages (0.8.3)\n",
      "Requirement already satisfied: geopandas in /usr/local/lib/python3.7/site-packages (0.4.1)\n",
      "Requirement already satisfied: shapely in /usr/local/lib/python3.7/site-packages (1.6.4.post2)\n",
      "Requirement already satisfied: sodapy in /usr/local/lib/python3.7/site-packages (1.5.2)\n",
      "Requirement already satisfied: python-dateutil>=2.5.0 in /usr/local/lib/python3.7/site-packages (from pandas==0.24.2) (2.8.0)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.7/site-packages (from pandas==0.24.2) (1.16.2)\n",
      "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.7/site-packages (from pandas==0.24.2) (2018.9)\n",
      "Requirement already satisfied: patsy in /usr/local/lib/python3.7/site-packages (from statsmodels==0.9.0) (0.5.1)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/site-packages (from rtree) (46.0.0)\n",
      "Requirement already satisfied: pyproj in /usr/local/lib/python3.7/site-packages (from geopandas) (2.1.3)\n",
      "Requirement already satisfied: fiona in /usr/local/lib/python3.7/site-packages (from geopandas) (1.8.6)\n",
      "Requirement already satisfied: future>=0.17.1 in /usr/local/lib/python3.7/site-packages (from sodapy) (0.17.1)\n",
      "Requirement already satisfied: requests>=2.20.0 in /usr/local/lib/python3.7/site-packages (from sodapy) (2.21.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/site-packages (from python-dateutil>=2.5.0->pandas==0.24.2) (1.12.0)\n",
      "Requirement already satisfied: munch in /usr/local/lib/python3.7/site-packages (from fiona->geopandas) (2.3.2)\n",
      "Requirement already satisfied: click<8,>=4.0 in /usr/local/lib/python3.7/site-packages (from fiona->geopandas) (7.0)\n",
      "Requirement already satisfied: cligj>=0.5 in /usr/local/lib/python3.7/site-packages (from fiona->geopandas) (0.5.0)\n",
      "Requirement already satisfied: click-plugins>=1.0 in /usr/local/lib/python3.7/site-packages (from fiona->geopandas) (1.1.1)\n",
      "Requirement already satisfied: attrs>=17 in /usr/local/lib/python3.7/site-packages (from fiona->geopandas) (19.1.0)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.7/site-packages (from requests>=2.20.0->sodapy) (2.8)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.7/site-packages (from requests>=2.20.0->sodapy) (1.24.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/site-packages (from requests>=2.20.0->sodapy) (2019.3.9)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/site-packages (from requests>=2.20.0->sodapy) (3.0.4)\n"
     ]
    }
   ],
   "source": [
    "!wget http://download.osgeo.org/libspatialindex/spatialindex-src-1.8.5.tar.gz\n",
    "!tar -zxf spatialindex-src-1.8.5.tar.gz\n",
    "!cd spatialindex-src-1.8.5\n",
    "!cd spatialindex-src-1.8.5 && ./configure && make && make install\n",
    "!ldconfig\n",
    "!pip install pandas==0.24.2 statsmodels==0.9.0 rtree geopandas shapely sodapy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading datasets into our notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start with two datasets as the basis of our analysis. We'll load a city of Seattle permits dataset as well as a shapefile with polygons defined for each of the neighborhoods in the state of Washington. We download these datasets into pandas DataFrames using the open source sodapy Python Socrata module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Requests made without an app_token will be subject to strict throttling limits.\n"
     ]
    },
    {
     "ename": "HTTPError",
     "evalue": "404 Client Error: Not Found.\n\tThis domain has been decommissioned.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-b663a26de614>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;31m# load shapefile data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0mwa_neighborhoods_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdownload_neighborhoods_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-b663a26de614>\u001b[0m in \u001b[0;36mdownload_neighborhoods_dataset\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdownload_neighborhoods_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mwa_neighborhoods_df\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m\"wa_neighborhoods_df\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mglobals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mdownload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"robo.demo.socrata.com\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"smef-bsgr\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m# load Seattle permits data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-b663a26de614>\u001b[0m in \u001b[0;36mdownload_dataset\u001b[0;34m(domain, dataset_id)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mrecords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moffset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/sodapy/__init__.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, dataset_identifier, content_type, **kwargs)\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m         response = self._perform_request(\"get\", resource, headers=headers,\n\u001b[0;32m--> 362\u001b[0;31m                                          params=params)\n\u001b[0m\u001b[1;32m    363\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/sodapy/__init__.py\u001b[0m in \u001b[0;36m_perform_request\u001b[0;34m(self, request_type, resource, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m         \u001b[0;31m# handle errors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m202\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0m_raise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m         \u001b[0;31m# when responses have no content body (ie. delete, set_permission),\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/sodapy/__init__.py\u001b[0m in \u001b[0;36m_raise_for_status\u001b[0;34m(response)\u001b[0m\n\u001b[1;32m    529\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmore_info\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmore_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreason\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mhttp_error_msg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\".\\n\\t{0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmore_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 531\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mHTTPError\u001b[0m: 404 Client Error: Not Found.\n\tThis domain has been decommissioned."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sodapy import Socrata\n",
    "\n",
    "def download_dataset(domain, dataset_id):\n",
    "    # for this exercise, we're not using an app token,\n",
    "    # but you *should* sign-up and register for an app_token if you want to use the Socrata API\n",
    "    client = Socrata(domain, app_token=None)\n",
    "    offset = None\n",
    "    data = []\n",
    "    batch_size = 1000\n",
    "\n",
    "    while True:\n",
    "        records = client.get(dataset_id, offset=offset, limit=batch_size)\n",
    "        data.extend(records)\n",
    "        if len(records) < batch_size:\n",
    "            break\n",
    "        offset = offset + batch_size if (offset) else batch_size\n",
    "\n",
    "    return pd.DataFrame.from_dict(data)\n",
    "\n",
    "def download_permits_dataset():\n",
    "    return seattle_permits_df if \"seattle_permits_df\" in globals() else download_dataset(\"data.seattle.gov\", \"k44w-2dcq\")\n",
    "\n",
    "def download_neighborhoods_dataset():\n",
    "    return wa_neighborhoods_df if \"wa_neighborhoods_df\" in globals() else download_dataset(\"robo.demo.socrata.com\", \"smef-bsgr\")\n",
    "\n",
    "# load Seattle permits data\n",
    "seattle_permits_df = download_permits_dataset()\n",
    "\n",
    "# load shapefile data\n",
    "wa_neighborhoods_df = download_neighborhoods_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next few cells we'll do some exploration of our datasets using the `len`, `head`, and `value_counts` functions. We'll start by getting a sense of how many rows are in each of our datasets with the `len` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "138396"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(seattle_permits_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see have a peek at the first 5 rows in each of those dataset using the `head` method. You can optionally pass a parameter for the number of rows you want to print if 5 isn't enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wa_neighborhoods_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-e50f9b231cca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwa_neighborhoods_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'wa_neighborhoods_df' is not defined"
     ]
    }
   ],
   "source": [
    "len(wa_neighborhoods_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wa_neighborhoods_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seattle_permits_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Any ideas about how we might print out the last N rows? Hint: enter the following on its own line and execute it: help(wa_neighborhoods_df.tail)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing our dataframes like this gives us a sense of what columns exist, and quick sense of some of the values in the dataset. But there's an even better way to detrmine the top values for a particular column -- the `value_counts` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seattle_permits_df[\"applieddate\"].value_counts(dropna=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value counts make it clear that a lot of the values in the \"applieddate\" column are missing or null. Since the crux of our analysis requires time and location data, we need to handle those missing values. There a variety of ways you can handle missing data, but removing incomplete rows is the simplest, so it's what we'll do here today. In the next cell, we'll remove rows with null dates and null latitude and longitude columns. There are also a lot of columns in the permits dataset that we won't use in this analysis. So we'll also filter down our dataset to just the columns we're interested in to reduce the amount of extraneous information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seattle_permits_df = seattle_permits_df[seattle_permits_df[\"applieddate\"].notnull()]\n",
    "seattle_permits_df = seattle_permits_df[seattle_permits_df[\"latitude\"].notnull()]\n",
    "seattle_permits_df = seattle_permits_df[seattle_permits_df[\"longitude\"].notnull()]\n",
    "seattle_permits_df = seattle_permits_df[[\"applieddate\", \"latitude\", \"longitude\"]].reset_index(drop=True)\n",
    "seattle_permits_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing place-based data with Socrata Maps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading and visualizing geographical data is relatively straight-forward in the Socrata UI. Before doing any additional work in Python, let's embed a map created in Socrata using its embed link. You'll notice that since the map is embedded within an IFrame (using iPython's IFrame class), that it's dynamic and we have preserved all the functionality from the map experience in the platform like changing the zoom, and searching for data within the map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"800\"\n",
       "            height=\"600\"\n",
       "            src=\"https://robo.demo.socrata.com/dataset/Seattle-Building-Permits-Point-Map/7unc-ff4h/embed?width=800&height=600\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x11ef8d050>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import IFrame\n",
    "IFrame('https://robo.demo.socrata.com/dataset/Seattle-Building-Permits-Point-Map/7unc-ff4h/embed?width=800&height=600', width=800, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The map above is a point map. Each of the rows from our permit dataset is rendered as a point on the map. Any observations about this visualization?\n",
    "\n",
    "I think we can do better with a multi-layer choropleth map -- also created within the Socrata platform -- where we aggregate our points based on the neighborhoods that they fall within."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"800\"\n",
       "            height=\"600\"\n",
       "            src=\"https://robo.demo.socrata.com/dataset/Seattle-Building-Permits-Choropleth/i6rm-ys8r/embed?width=800&height=600\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x110468210>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import IFrame\n",
    "IFrame(\"https://robo.demo.socrata.com/dataset/Seattle-Building-Permits-Choropleth/i6rm-ys8r/embed?width=800&height=600\", width=800, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any observations about this choropleth map?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepping our geographic columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like in our choropleth map, we'll try to group our data by neighborhood, and we'll try to improve on our map by looking at how our permit applications change over time.\n",
    "\n",
    "Notice that the neighborhood shapefile we imported from Socrata contained boundaries for all neighborhoods in the state of Washington. (Can you use the the `value_counts` method to determine the top cities in the `wa_neighborhoods_df` dataset?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can you use the the value_counts method to determine the top cities in the wa_neighborhoods_df dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's filter that down to the city of Seattle, since that's the focus of this analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seattle_neighborhoods_df = wa_neighborhoods_df[wa_neighborhoods_df[\"city\"] == \"Seattle\"].reset_index(drop=True)\n",
    "seattle_neighborhoods_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can you use the `len` method to determine the number of rows in our new seattle_neighborhoods_df dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the really powerful features of notebooks is that you can execute arbitrary code to transform your data. Since data wrangling is one of the fundamental tasks in any data analysis project, it's essential that we have tools to reshape and slice our data. This is one of the strengths of the pandas in Python.\n",
    "\n",
    "We have already loaded the underlying neighborhood shapes, but let's make our coordinate and shape columns a little more useful by changing them to types that are a little easier to work with in Python. We'll make use of the geopandas and shapely Python modules. Why are we doing this? We want a single geographic column in each of our datasets, and in order to perform a join on those geographic columns, we need them to be respresented as types that our join function understands."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell, we'll add a new Polygon column to our shapefile dataset. We'll write a function that takes a shape that's encoded in the geojson format, and transform it into a Polygon column. We'll use the `apply` method to do this transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.geometry import Polygon\n",
    "\n",
    "def polygon_from_geojson(geojson):\n",
    "    return Polygon(geojson[\"coordinates\"][0][0])\n",
    "\n",
    "seattle_neighborhoods_df[\"polygon\"] = seattle_neighborhoods_df[\"the_geom\"].apply(polygon_from_geojson)\n",
    "seattle_neighborhoods_df[\"polygon\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll add a new Point column to our permits dataset. We write a function that takes an entire row from our dataset as an input, pulls out our latitude and longitude from that row, and returns a Point corresponding to those coordinates. Once again, we'll use the `apply` method to do this transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.geometry import Point\n",
    "\n",
    "def point_from_coordinates(row):\n",
    "    coordinates = (row[\"longitude\"], row[\"latitude\"])\n",
    "    return Point(coordinates)\n",
    "\n",
    "seattle_permits_df[\"latitude\"] = pd.to_numeric(seattle_permits_df[\"latitude\"])\n",
    "seattle_permits_df[\"longitude\"] = pd.to_numeric(seattle_permits_df[\"longitude\"])\n",
    "seattle_permits_df[\"point\"] = seattle_permits_df.apply(point_from_coordinates, axis=1)\n",
    "seattle_permits_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing a spatial join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we our landuse dataframe and our neighborhoods dataframe, each with geometry columns, that next thing we want to do is perform a spatial join so we can have a single dataframe tying permits to neighborhoods. This will allow us to compare across neighborhoods, just as we did with the heat map earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "permits_geo_df = gpd.GeoDataFrame(seattle_permits_df, geometry='point')\n",
    "neighborhoods_geo_df = gpd.GeoDataFrame(seattle_neighborhoods_df, geometry='polygon')\n",
    "\n",
    "joined_df = pd.DataFrame(gpd.sjoin(permits_geo_df, neighborhoods_geo_df, how=\"inner\", op=\"within\", rsuffix=\"neighborhood_\"))\n",
    "joined_df = joined_df[[\"applieddate\", \"name\"]]\n",
    "\n",
    "## TODO: print the first rows of the joined data frame `joined_df`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregating based on neighborhood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neat! Now that we have a single dataset with each row corresponding to a permit and its corresponding neighborhood, we can group our dataset by neighborhood, which is the first step to exploring the number of permits by neighborhood. To make our neighborhood-based analysis a little more digestible, we'll restrict it to a subset of Seattle neighborhoods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighborhoods_of_interest = set([\"Ballard\", \"Capitol Hill\", \"Magnolia\", \"South Lake Union\", \"North Beacon Hill\", \"Pioneer Square\"])\n",
    "\n",
    "def is_neighborhood_of_interest(neighborhood):\n",
    "    return neighborhood in neighborhoods_of_interest\n",
    "\n",
    "subset_df = joined_df[joined_df[\"name\"].apply(is_neighborhood_of_interest)].reset_index(drop=True)\n",
    "subset_df.groupby(\"name\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregating based on neighborhood and date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having grouped our data based on neighborhood, the next thing we want to do is look at one aspect of the data that was missing from the choropleth map -- time. Just as we did with the `Point` and `Polygon` columns previously, we need to convert the `applieddate` column type in the permits dataframe to a `datetime` in order to benefit from some time-based functionality in Python. We'll also filter out dates before 2005 and during 2019 so the years are comparable. We also make the new `datetime` column the index column for our dataframe, which gives us some added benefits such as automatic sorting based on date and good default behavior when plotting our time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "def is_between_2005_and_2018(date):\n",
    "    return date > datetime.datetime(2005, 1, 1) and date < datetime.datetime(2019, 1, 1)\n",
    "\n",
    "fixed_dates_df = subset_df.copy()\n",
    "fixed_dates_df[\"applieddate\"] = subset_df[\"applieddate\"].apply(pd.to_datetime)\n",
    "fixed_dates_df = fixed_dates_df[fixed_dates_df[\"applieddate\"].apply(is_between_2005_and_2018)]\n",
    "fixed_dates_df = fixed_dates_df.set_index(fixed_dates_df[\"applieddate\"]).drop([\"applieddate\"], axis=1)\n",
    "grouped = fixed_dates_df.groupby(\"name\").resample(\"M\").count()\n",
    "grouped.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting a time series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, with our data grouped by neighborhood and by month, let's visualize it! The code below is somewhat complex, so we won't go into it in detail. But the gist is this: we iterate over our groups (neighborhoods) and create a time series plot for each, labeling each plot based on its name, and coloring our line purple just because who doesn't like purple?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "register_matplotlib_converters()\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "ncols=2\n",
    "nrows=3\n",
    "\n",
    "fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(15,15))\n",
    "\n",
    "for (name, neighborhood_df), axis in zip(grouped.groupby(level=0), axes.flatten()):\n",
    "    axis.xaxis.label.set_visible(False)\n",
    "    axis.set_ylim(0, 75)\n",
    "    axis.set_xlim(left=datetime.date(2000, 1, 1), right=datetime.date(2018, 12, 31))\n",
    "    axis.yaxis.set_major_formatter(FormatStrFormatter('%g'))\n",
    "    neighborhood_df.xs(name).plot(title=name, style=\"\", ax=axis, legend=False, color=\"purple\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are your observations based on these plots?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding trend and seasonality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's select one of these neighboorhoods to drill in on. I'm partial to Ballard.\n",
    "\n",
    "The statsmodels package in Python gives us a really useful set of tools for analyzing time series data. In particular, we can use the `seasonal_decompose` method to breakdown a time series into consituent series corresponding to the longterm trend in the data, the seasonal aspect of the data, and noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "name, neighborhood_df = list(grouped.groupby(level=0))[0] # Ballard\n",
    "result = seasonal_decompose(neighborhood_df.xs(name))\n",
    "fig = result.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Takeaways"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So where do we go from here? Identifying the seasonal aspect of permit applications might be useful in its own right. It might provide some guidance for determining when to increase reasources in the permitting department. Another interesting investigation might be trying to forecast permit applications by neighborhood.\n",
    "\n",
    "We've seen how pulling our datasets geographic and time series data into a Jupyter Notebook can lead to insights that may be beyond our reach within the Socrata platform. In the year ahead, one exciting area of investment for the Data & Insights division will be enabling our users to create in-platform notebooks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
